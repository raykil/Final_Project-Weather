{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "from matplotlib_inline import backend_inline\n",
    "backend_inline.set_matplotlib_formats('retina')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 11: Introduction to Deep Neural Networks with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "The goal of this notebook is to introduce deep neural networks (DNNs) using the high-level Keras package. The reader will become familiar with how to choose an architecture, cost function, and optimizer in Keras. We will also learn how to train neural networks.\n",
    "\n",
    "\n",
    "# MNIST with Keras\n",
    "\n",
    "We will once again work with the MNIST dataset of hand written digits introduced in *Notebook 7: Logistic Regression (MNIST)*. The goal is to find a statistical model which recognizes and distinguishes between the ten handwritten digits (0-9).\n",
    "\n",
    "The MNIST dataset comprises $70000$ handwritten digits, each of which comes in a square image, divided into a $28\\times 28$ pixel grid. Every pixel can take on $256$ nuances of the gray color, interpolating between white and black, and hence each data point assumes any value in the set $\\{0,1,\\dots,255\\}$. Since there are $10$ categories in the problem, corresponding to the ten digits, this problem represents a generic classification task. \n",
    "\n",
    "In this Notebook, we show how to use the Keras python package to tackle the MNIST problem with the help of deep neural networks.\n",
    "\n",
    "The following code is a slight modification of a Keras tutorial, see [https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py](https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py). We invite the reader to read Sec. IX of the review to acquire a broad understanding of what the separate parts of the code do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 20:45:10.712201: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras,sklearn\n",
    "# suppress tensorflow compilation warnings\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import numpy as np\n",
    "seed=0\n",
    "np.random.seed(seed) # fix random seed\n",
    "tf.random.set_seed(seed)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of the Procedure\n",
    "\n",
    "Constructing a Deep Neural Network to solve ML problems is a multiple-stage process. Quite generally, one can identify the key steps as follows:\n",
    "\n",
    "* ***step 1:*** Load and process the data\n",
    "* ***step 2:*** Define the model and its architecture\n",
    "* ***step 3:*** Choose the optimizer and the cost function\n",
    "* ***step 4:*** Train the model \n",
    "* ***step 5:*** Evaluate the model performance on the *unseen* test data\n",
    "* ***step 6:*** Modify the hyperparameters to optimize performance for the specific data set\n",
    "\n",
    "We would like to emphasize that, while it is always possible to view steps 1-5 as independent of the particular task we are trying to solve, it is only when they are put together in ***step 6*** that the real gain of using Deep Learning is revealed, compared to less sophisticated methods such as the regression models or bagging, described in Secs. VII and VIII of the review. With this remark in mind, we shall focus predominantly on steps 1-5 below. We show how one can use grid search methods to find optimal hyperparameters in ***step 6***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load and Process the Data\n",
    "\n",
    "Keras can conveniently download the MNIST data from the web. All we need to do is import the `mnist` module and use the `load_data()` class, and it will create the training and test data sets or us.\n",
    "\n",
    "The MNIST set has pre-defined test and training sets, in order to facilitate the comparison of the performance of different models on the data.\n",
    "\n",
    "Once we have loaded the data, we need to format it in the correct shape. This differs from one package to the other and, as we see in the case of Keras, it can even be different depending on the backend used.\n",
    "\n",
    "While choosing the correct `datatype` can help improve the computational speed, we emphasize the rescaling step, which is necessary to avoid large variations in the minimal and maximal possible values of each feature. In other words, we want to make sure a feature is not being over-represented just because it is \"large\".\n",
    "\n",
    "Last, we cast the label vectors $y$ to binary class matrices (a.k.a. one-hot format), as explained in Sec. VII on SoftMax regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an example of a data point with label 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 419
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 784)\n",
      "Y_train shape: (60000, 10)\n",
      "\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "# input image dimensions\n",
    "num_classes = 10 # 10 digits\n",
    "\n",
    "img_rows, img_cols = 28, 28 # number of pixels \n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "# reshape data, depending on Keras backend\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows*img_cols)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows*img_cols)\n",
    "    \n",
    "# cast floats to single precesion\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# rescale data in interval [0,1]\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# look at an example of data point\n",
    "print('an example of a data point with label', Y_train[20])\n",
    "plt.matshow(X_train[20,:].reshape(28,28),cmap='binary')\n",
    "plt.show()\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = keras.utils.to_categorical(Y_train, num_classes)\n",
    "Y_test = keras.utils.to_categorical(Y_test, num_classes)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print()\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define the Neural Net and its Architecture\n",
    "\n",
    "We can now move on to construct our deep neural net. We shall use Keras's `Sequential()` class to instantiate a model, and will add different deep layers one by one.\n",
    "\n",
    "At this stage, we refrain from using convolutional layers. This is done further below.\n",
    "\n",
    "Let us create an instance of Keras' `Sequential()` class, called `model`. As the name suggests, this class allows us to build DNNs layer by layer. We use the `add()` method to attach layers to our model. For the purposes of our introductory example, it suffices to focus on `Dense` layers for simplicity. Every `Dense()` layer accepts as its first required argument an integer which specifies the number of neurons. The type of activation function for the layer is defined using the `activation` optional argument, the input of which is the name of the activation function in `string` format. Examples include `relu`, `tanh`, `elu`, `sigmoid`, `softmax`. \n",
    "\n",
    "In order for our DNN to work properly, we have to make sure that the numbers of input and output neurons for each layer match. Therefore, we specify the shape of the input in the first layer of the model explicitly using the optional argument `input_shape=(N_features,)`. The sequential construction of the model then allows Keras to infer the correct input/output dimensions of all hidden layers automatically. Hence, we only need to specify the size of the softmax output layer to match the number of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture created successfully!\n"
     ]
    }
   ],
   "source": [
    "def create_DNN():\n",
    "    # instantiate model\n",
    "    model = Sequential()\n",
    "    # add a dense all-to-all relu layer\n",
    "    model.add(Dense(400,input_shape=(img_rows*img_cols,), activation='relu'))\n",
    "    # add a dense all-to-all relu layer\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    # apply dropout with rate 0.5\n",
    "    model.add(Dropout(0.5))\n",
    "    # soft-max layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "print('Model architecture created successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Choose the Optimizer and the Cost Function\n",
    "\n",
    "Next, we choose the loss function according to which to train the DNN. For classification problems, this is the cross entropy, and since the output data was cast in categorical form, we choose the `categorical_crossentropy` defined in Keras' `losses` module. Depending on the problem of interest one can pick any other suitable loss function. To optimize the weights of the net, we choose SGD. This algorithm is already available to use under Keras' `optimizers` module, but we could use `Adam()` or any other built-in one as well. The parameters for the optimizer, such as `lr` (learning rate) or `momentum` are passed using the corresponding optional arguments of the `SGD()` function. All available arguments can be found in Keras' online documentation at [https://keras.io/](https://keras.io/). While the loss function and the optimizer are essential for the training procedure, to test the performance of the model one may want to look at a particular `metric` of performance. For instance, in categorical tasks one typically looks at their `accuracy`, which is defined as the percentage of correctly classified data points. To complete the definition of our model, we use the `compile()` method, with optional arguments for the `optimizer`, `loss`, and the validation `metric` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled successfully and ready to be trained.\n"
     ]
    }
   ],
   "source": [
    "def compile_model(optimizer=keras.optimizers.Adam()):\n",
    "    # create the mode\n",
    "    model=create_DNN()\n",
    "    # compile the model\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "print('Model compiled successfully and ready to be trained.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train the model\n",
    "\n",
    "We train our DNN in minibatches, the advantages of which were explained in Sec. IV. \n",
    "\n",
    "Shuffling the training data during training improves stability of the model. Thus, we train over a number of training epochs. \n",
    "\n",
    "Training the DNN is a one-liner using the `fit()` method of the `Sequential` class. The first two required arguments are the training input and output data. As optional arguments, we specify the mini-`batch_size`, the number of training `epochs`, and the test or `validation_data`. To monitor the training procedure for every epoch, we set `verbose=True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 3s 3ms/step - loss: 0.3049 - accuracy: 0.9100 - val_loss: 0.1116 - val_accuracy: 0.9648\n"
     ]
    }
   ],
   "source": [
    "# training parameters\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "\n",
    "# create the deep neural net\n",
    "model_DNN=compile_model()\n",
    "\n",
    "# train DNN and store training info in history\n",
    "history=model_DNN.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Evaluate the Model Performance on the *Unseen* Test Data\n",
    "\n",
    "Next, we evaluate the model and read of the loss on the test data, and its accuracy using the `evaluate()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1116 - accuracy: 0.9648\n",
      "\n",
      "Test loss: 0.1115909069776535\n",
      "Test accuracy: 0.9648000001907349\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# summarize history for accuracy\\nplt.plot(history.history['accuracy'])\\nplt.plot(history.history['val_accuracy'])\\nplt.ylabel('model accuracy')\\nplt.xlabel('epoch')\\nplt.legend(['train', 'test'], loc='best')\\nplt.show()\\n\\n# summarize history for loss\\nplt.plot(history.history['loss'])\\nplt.plot(history.history['val_loss'])\\nplt.ylabel('model loss')\\nplt.xlabel('epoch')\\nplt.legend(['train', 'test'], loc='best')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate model\n",
    "\n",
    "score = model_DNN.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# look into training history\n",
    "'''\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.ylabel('model accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('model loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "400 -> 200 -> 10, 10 epochs -- 0.9799\n",
    "400 -> 200 -> 10, 1 epoch -- 0.9614\n",
    "4000 -> 10, 1 epoch -- 0.9685\n",
    "4000 -> 1000 -> 1000 -> 10, 1 epoch -- 0.9625\n",
    "4000 -> 1000 -> 100 -> 10, 1 epoch -- 0.9656"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 6: Modify the Hyperparameters to Optimize Performance of the Model\n",
    "\n",
    "Last, we show how to use the grid search option of scikit-learn to optimize the \n",
    "hyperparameters of our model. An excellent blog on this by Jason Brownlee can be found on [https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/](https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/5382fyf919n7cf9211lb816c0000gn/T/ipykernel_54820/3408251807.py:14: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model_gridsearch = KerasClassifier(build_fn=compile_model,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704/704 [==============================] - 2s 3ms/step - loss: 1.1616 - accuracy: 0.6532\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 0.5185 - accuracy: 0.8728\n",
      "704/704 [==============================] - 2s 3ms/step - loss: 1.1499 - accuracy: 0.6538\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 0.5180 - accuracy: 0.8697\n",
      "289/704 [===========>..................] - ETA: 1s - loss: 1.5960 - accuracy: 0.5092"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [81], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# call scikit grid search module\u001b[39;00m\n\u001b[1;32m     24\u001b[0m grid \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mmodel_gridsearch, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m grid_result \u001b[38;5;241m=\u001b[39m \u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# summarize results\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m using \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (grid_result\u001b[38;5;241m.\u001b[39mbest_score_, grid_result\u001b[38;5;241m.\u001b[39mbest_params_))\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    869\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    871\u001b[0m     )\n\u001b[1;32m    873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 875\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    879\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1379\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1379\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/model_selection/_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    819\u001b[0m         )\n\u001b[1;32m    820\u001b[0m     )\n\u001b[0;32m--> 822\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    844\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/utils/fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/keras/wrappers/scikit_learn.py:248\u001b[0m, in \u001b[0;36mKerasClassifier.fit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape for y: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_)\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/keras/wrappers/scikit_learn.py:175\u001b[0m, in \u001b[0;36mBaseWrapper.fit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m fit_args \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_sk_params(Sequential\u001b[38;5;241m.\u001b[39mfit))\n\u001b[1;32m    173\u001b[0m fit_args\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[0;32m--> 175\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def compile_model(optimizer=keras.optimizers.Adam()):\n",
    "    # create the mode\n",
    "    model=create_DNN()\n",
    "    # compile the model\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# call Keras scikit wrapper\n",
    "model_gridsearch = KerasClassifier(build_fn=compile_model, \n",
    "                        epochs=1, \n",
    "                        batch_size=batch_size, \n",
    "                        verbose=1)\n",
    "\n",
    "# list of allowed optional arguments for the optimizer, see `compile_model()`\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "# define parameter dictionary\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "# call scikit grid search module\n",
    "grid = GridSearchCV(estimator=model_gridsearch, param_grid=param_grid, n_jobs=1, cv=4)\n",
    "grid_result = grid.fit(X_train,Y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_DNN(nlayers=2, layer1size=400):\n",
    "    # instantiate model\n",
    "    model = Sequential()\n",
    "    # add a dense all-to-all relu layer\n",
    "    model.add(Dense(layer1size,input_shape=(img_rows*img_cols,), activation='relu'))\n",
    "    # add a dense all-to-all relu layer\n",
    "    model.add(Dense(200, activation='relu'))\n",
    "    for i in range(nlayers-2):\n",
    "        model.add(Dense(100, activation='relu'))\n",
    "    # apply dropout with rate 0.5\n",
    "    model.add(Dropout(0.5))\n",
    "    # soft-max layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/5382fyf919n7cf9211lb816c0000gn/T/ipykernel_54820/3120406146.py:15: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model_gridsearch = KerasClassifier(build_fn=compile_model,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704/704 [==============================] - 18s 24ms/step - loss: 0.2621 - accuracy: 0.9218\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1322 - accuracy: 0.9609\n",
      "704/704 [==============================] - 18s 24ms/step - loss: 0.2672 - accuracy: 0.9206\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1203 - accuracy: 0.9620\n",
      "704/704 [==============================] - 17s 24ms/step - loss: 0.2661 - accuracy: 0.9210\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1314 - accuracy: 0.9612\n",
      "704/704 [==============================] - 19s 26ms/step - loss: 0.2673 - accuracy: 0.9214\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1247 - accuracy: 0.9623\n",
      "704/704 [==============================] - 19s 26ms/step - loss: 0.2613 - accuracy: 0.9222\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1235 - accuracy: 0.9633\n",
      "704/704 [==============================] - 20s 25ms/step - loss: 0.2771 - accuracy: 0.9187\n",
      "235/235 [==============================] - 2s 5ms/step - loss: 0.1215 - accuracy: 0.9637\n",
      "704/704 [==============================] - 21s 29ms/step - loss: 0.2569 - accuracy: 0.9235\n",
      "235/235 [==============================] - 2s 5ms/step - loss: 0.1296 - accuracy: 0.9611\n",
      "704/704 [==============================] - 19s 26ms/step - loss: 0.2714 - accuracy: 0.9195\n",
      "235/235 [==============================] - 2s 5ms/step - loss: 0.1288 - accuracy: 0.9611\n",
      "704/704 [==============================] - 19s 26ms/step - loss: 0.2555 - accuracy: 0.9251\n",
      "235/235 [==============================] - 2s 5ms/step - loss: 0.1198 - accuracy: 0.9643\n",
      "704/704 [==============================] - 20s 27ms/step - loss: 0.2744 - accuracy: 0.9193\n",
      "235/235 [==============================] - 2s 6ms/step - loss: 0.1279 - accuracy: 0.9614\n",
      "704/704 [==============================] - 22s 29ms/step - loss: 0.2562 - accuracy: 0.9241\n",
      "235/235 [==============================] - 2s 5ms/step - loss: 0.1266 - accuracy: 0.9615\n",
      "704/704 [==============================] - 21s 29ms/step - loss: 0.2603 - accuracy: 0.9220\n",
      "235/235 [==============================] - 2s 6ms/step - loss: 0.1250 - accuracy: 0.9623\n",
      "704/704 [==============================] - 3s 4ms/step - loss: 0.3203 - accuracy: 0.9039\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1571 - accuracy: 0.9531\n",
      "704/704 [==============================] - 4s 4ms/step - loss: 0.3175 - accuracy: 0.9053\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 0.1513 - accuracy: 0.9557\n",
      "704/704 [==============================] - 3s 4ms/step - loss: 0.3199 - accuracy: 0.9052\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1676 - accuracy: 0.9497\n",
      "704/704 [==============================] - 4s 4ms/step - loss: 0.3222 - accuracy: 0.9040\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 0.1533 - accuracy: 0.9540\n",
      "704/704 [==============================] - 3s 4ms/step - loss: 0.3007 - accuracy: 0.9094\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 0.1555 - accuracy: 0.9534\n",
      "704/704 [==============================] - 3s 4ms/step - loss: 0.3015 - accuracy: 0.9091\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 0.1391 - accuracy: 0.9587\n",
      "704/704 [==============================] - 3s 4ms/step - loss: 0.3008 - accuracy: 0.9115\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 0.1551 - accuracy: 0.9519\n",
      "704/704 [==============================] - 3s 4ms/step - loss: 0.3068 - accuracy: 0.9090\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 0.1440 - accuracy: 0.9567\n",
      "704/704 [==============================] - 3s 4ms/step - loss: 0.2880 - accuracy: 0.9132\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1373 - accuracy: 0.9581\n",
      "704/704 [==============================] - 3s 4ms/step - loss: 0.2911 - accuracy: 0.9128\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1383 - accuracy: 0.9595\n",
      "704/704 [==============================] - 4s 4ms/step - loss: 0.2785 - accuracy: 0.9155\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1517 - accuracy: 0.9539\n",
      "704/704 [==============================] - 4s 4ms/step - loss: 0.2872 - accuracy: 0.9130\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 0.1432 - accuracy: 0.9567\n",
      "938/938 [==============================] - 27s 28ms/step - loss: 0.2299 - accuracy: 0.9329\n",
      "Best: 0.962400 using {'layer1size': 4000, 'learning_rate': 0.003981071705534973, 'nlayers': 2, 'optimizer': <class 'keras.optimizers.optimizer_v2.adamax.Adamax'>}\n",
      "0.961600 (0.000591) with: {'layer1size': 4000, 'learning_rate': 0.0025118864315095794, 'nlayers': 2, 'optimizer': <class 'keras.optimizers.optimizer_v2.adamax.Adamax'>}\n",
      "0.962300 (0.001212) with: {'layer1size': 4000, 'learning_rate': 0.0031622776601683794, 'nlayers': 2, 'optimizer': <class 'keras.optimizers.optimizer_v2.adamax.Adamax'>}\n",
      "0.962400 (0.001172) with: {'layer1size': 4000, 'learning_rate': 0.003981071705534973, 'nlayers': 2, 'optimizer': <class 'keras.optimizers.optimizer_v2.adamax.Adamax'>}\n",
      "0.953117 (0.002190) with: {'layer1size': 400, 'learning_rate': 0.0025118864315095794, 'nlayers': 2, 'optimizer': <class 'keras.optimizers.optimizer_v2.adamax.Adamax'>}\n",
      "0.955167 (0.002679) with: {'layer1size': 400, 'learning_rate': 0.0031622776601683794, 'nlayers': 2, 'optimizer': <class 'keras.optimizers.optimizer_v2.adamax.Adamax'>}\n",
      "0.957033 (0.002079) with: {'layer1size': 400, 'learning_rate': 0.003981071705534973, 'nlayers': 2, 'optimizer': <class 'keras.optimizers.optimizer_v2.adamax.Adamax'>}\n"
     ]
    }
   ],
   "source": [
    "def compile_model(optimizer=keras.optimizers.Adam, learning_rate=1e-3, nlayers=2, layer1size=400):\n",
    "    # create the mode\n",
    "    model=create_new_DNN(nlayers=nlayers, layer1size=layer1size)\n",
    "    # compile the model\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=optimizer(learning_rate=learning_rate),\n",
    "                  metrics=['accuracy'], \n",
    "                  )\n",
    "    return model\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# call Keras scikit wrapper\n",
    "model_gridsearch = KerasClassifier(build_fn=compile_model, \n",
    "                        epochs=1, \n",
    "                        batch_size=batch_size, \n",
    "                        verbose=1)\n",
    "\n",
    "# list of allowed optional arguments for the optimizer, see `compile_model()`\n",
    "#optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "#optimizer = [keras.optimizers.Adam, keras.optimizers.SGD, keras.optimizers.Nadam, keras.optimizers.RMSprop, \\\n",
    "#             keras.optimizers.Adagrad, keras.optimizers.Adadelta, keras.optimizers.Adamax]\n",
    "optimizer = [keras.optimizers.Adamax]\n",
    "learn_rate = np.logspace(-2.6, -2.4, 3)\n",
    "nlayers = [2]\n",
    "layer1sizes=[4000, 400]\n",
    "\n",
    "# define parameter dictionary\n",
    "param_grid = dict(optimizer=optimizer, learning_rate=learn_rate, nlayers=nlayers, layer1size=layer1sizes)\n",
    "#param_grid = dict(optimizer=optimizer)\n",
    "# call scikit grid search module\n",
    "grid = GridSearchCV(estimator=model_gridsearch, param_grid=param_grid, n_jobs=1, cv=4)\n",
    "grid_result = grid.fit(X_train,Y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Best: 0.980283 using {'learning_rate': 0.0031622776601683794, 'nlayers': 2, 'optimizer': <class 'keras.optimizers.optimizer_v2.adamax.Adamax'>}\n",
    "0.976817 (0.002061) with: {'learning_rate': 0.001, 'nlayers': 2, 'optimizer': <class 'keras.optimizers.optimizer_v2.nadam.Nadam'>}\n",
    "0.978367 (0.001239) with: {'learning_rate': 0.001, 'nlayers': 2, 'optimizer': <class 'keras.optimizers.optimizer_v2.adamax.Adamax'>}\n",
    "0.975883 (0.003149) with: {'learning_rate': 0.001, 'nlayers': 3, 'optimizer': <class 'keras.optimizers.optimizer_v2.nadam.Nadam'>}\n",
    "0.977867 (0.000598) with: {'learning_rate': 0.001, 'nlayers': 3, 'optimizer': <class 'keras.optimizers.optimizer_v2.adamax.Adamax'>}\n",
    "0.973917 (0.001712) with: {'learning_rate': 0.0031622776601683794, 'nlayers': 2, 'optimizer': <class 'keras.optimizers.optimizer_v2.nadam.Nadam'>}\n",
    "0.980283 (0.001603) with: {'learning_rate': 0.0031622776601683794, 'nlayers': 2, 'optimizer': <class 'keras.optimizers.optimizer_v2.adamax.Adamax'>}\n",
    "0.974933 (0.002102) with: {'learning_rate': 0.0031622776601683794, 'nlayers': 3, 'optimizer': <class 'keras.optimizers.optimizer_v2.nadam.Nadam'>}\n",
    "0.979450 (0.001546) with: {'learning_rate': 0.0031622776601683794, 'nlayers': 3, 'optimizer': <class 'keras.optimizers.optimizer_v2.adamax.Adamax'>}\n",
    "0.956050 (0.004905) with: {'learning_rate': 0.01, 'nlayers': 2, 'optimizer': <class 'keras.optimizers.optimizer_v2.nadam.Nadam'>}\n",
    "0.978217 (0.001213) with: {'learning_rate': 0.01, 'nlayers': 2, 'optimizer': <class 'keras.optimizers.optimizer_v2.adamax.Adamax'>}\n",
    "0.959467 (0.003262) with: {'learning_rate': 0.01, 'nlayers': 3, 'optimizer': <class 'keras.optimizers.optimizer_v2.nadam.Nadam'>}\n",
    "0.977850 (0.000488) with: {'learning_rate': 0.01, 'nlayers': 3, 'optimizer': <class 'keras.optimizers.optimizer_v2.adamax.Adamax'>}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Best: 0.980483 using {'learning_rate': 0.0031622776601683794, 'nlayers': 2, 'optimizer': <class 'keras.optimizers.optimizer_v2.adamax.Adamax'>}\n",
    "0.979300 (0.001450) with: {'learning_rate': 0.0017782794100389228, 'nlayers': 2, 'optimizer': <class 'keras.optimizers.optimizer_v2.adamax.Adamax'>}\n",
    "0.978333 (0.000568) with: {'learning_rate': 0.0017782794100389228, 'nlayers': 5, 'optimizer': <class 'keras.optimizers.optimizer_v2.adamax.Adamax'>}\n",
    "0.980483 (0.000682) with: {'learning_rate': 0.0031622776601683794, 'nlayers': 2, 'optimizer': <class 'keras.optimizers.optimizer_v2.adamax.Adamax'>}\n",
    "0.979283 (0.002039) with: {'learning_rate': 0.0031622776601683794, 'nlayers': 5, 'optimizer': <class 'keras.optimizers.optimizer_v2.adamax.Adamax'>}\n",
    "0.979633 (0.001136) with: {'learning_rate': 0.005623413251903491, 'nlayers': 2, 'optimizer': <class 'keras.optimizers.optimizer_v2.adamax.Adamax'>}\n",
    "0.976533 (0.000406) with: {'learning_rate': 0.005623413251903491, 'nlayers': 5, 'optimizer': <class 'keras.optimizers.optimizer_v2.adamax.Adamax'>}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Best so far is \n",
    "    : 0.980483 using {'learning_rate': 0.0031622776601683794, 'nlayers': 2, 'optimizer': <class 'keras.optimizers.optimizer_v2.adamax.Adamax'>},  15 epochs\n",
    "    \n",
    "Currently running same as above but with 4000 size layer and 400 with only 1 epoch. If 4000 is best, then I'll run with 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DNN.fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "938/938 [==============================] - 54s 56ms/step - loss: 0.2302 - accuracy: 0.9327 - val_loss: 0.0877 - val_accuracy: 0.9719\n",
      "Epoch 2/15\n",
      "938/938 [==============================] - 53s 56ms/step - loss: 0.0936 - accuracy: 0.9729 - val_loss: 0.0762 - val_accuracy: 0.9764\n",
      "Epoch 3/15\n",
      "938/938 [==============================] - 56s 60ms/step - loss: 0.0630 - accuracy: 0.9809 - val_loss: 0.0639 - val_accuracy: 0.9805\n",
      "Epoch 4/15\n",
      "938/938 [==============================] - 60s 64ms/step - loss: 0.0419 - accuracy: 0.9876 - val_loss: 0.0571 - val_accuracy: 0.9834\n",
      "Epoch 5/15\n",
      "938/938 [==============================] - 58s 62ms/step - loss: 0.0295 - accuracy: 0.9910 - val_loss: 0.0665 - val_accuracy: 0.9824\n",
      "Epoch 6/15\n",
      "938/938 [==============================] - 59s 63ms/step - loss: 0.0225 - accuracy: 0.9931 - val_loss: 0.0693 - val_accuracy: 0.9818\n",
      "Epoch 7/15\n",
      "938/938 [==============================] - 61s 65ms/step - loss: 0.0171 - accuracy: 0.9945 - val_loss: 0.0677 - val_accuracy: 0.9830\n",
      "Epoch 8/15\n",
      "938/938 [==============================] - 62s 66ms/step - loss: 0.0132 - accuracy: 0.9960 - val_loss: 0.0640 - val_accuracy: 0.9857\n",
      "Epoch 9/15\n",
      "938/938 [==============================] - 65s 69ms/step - loss: 0.0105 - accuracy: 0.9966 - val_loss: 0.0641 - val_accuracy: 0.9859\n",
      "Epoch 10/15\n",
      "938/938 [==============================] - 64s 69ms/step - loss: 0.0094 - accuracy: 0.9968 - val_loss: 0.0790 - val_accuracy: 0.9832\n",
      "Epoch 11/15\n",
      "938/938 [==============================] - 67s 72ms/step - loss: 0.0070 - accuracy: 0.9980 - val_loss: 0.0675 - val_accuracy: 0.9858\n",
      "Epoch 12/15\n",
      "938/938 [==============================] - 63s 68ms/step - loss: 0.0056 - accuracy: 0.9981 - val_loss: 0.0858 - val_accuracy: 0.9842\n",
      "Epoch 13/15\n",
      "938/938 [==============================] - 58s 62ms/step - loss: 0.0059 - accuracy: 0.9981 - val_loss: 0.0714 - val_accuracy: 0.9855\n",
      "Epoch 14/15\n",
      "938/938 [==============================] - 60s 64ms/step - loss: 0.0046 - accuracy: 0.9985 - val_loss: 0.0882 - val_accuracy: 0.9847\n",
      "Epoch 15/15\n",
      "938/938 [==============================] - 61s 65ms/step - loss: 0.0051 - accuracy: 0.9984 - val_loss: 0.0851 - val_accuracy: 0.9851\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.0851 - accuracy: 0.9851\n",
      "\n",
      "Test loss: 0.0851498618721962\n",
      "Test accuracy: 0.9850999712944031\n"
     ]
    }
   ],
   "source": [
    "# create the deep neural net\n",
    "model_DNN=compile_model(optimizer=keras.optimizers.Adamax, learning_rate=0.0031622776601683794, nlayers=2, layer1size=10000)\n",
    "\n",
    "# train DNN and store training info in history\n",
    "history=model_DNN.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "score = model_DNN.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2 layers, 4000, 15 epochs, Adamax, learning_rate 0.0031622776601683794\n",
    "\n",
    "-- Test loss: 0.0770016461610794\n",
    "-- Test accuracy: 0.9850000143051147\n",
    "\n",
    "3 layers, 4000, 15 epochs, Adamax, learning_rate 0.0031622776601683794\n",
    "\n",
    "-- Test loss: 0.0770016461610794\n",
    "-- Test accuracy: 0.9845000143051147\n",
    "\n",
    "2 layers, 10000, 15 epochs, Adamax, learning_rate 0.0031622776601683794\n",
    "\n",
    "-- Test loss: 0.0851498618721962\n",
    "-- Test accuracy: 0.9850999712944031\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Convolutional Neural Nets with Keras\n",
    "\n",
    "We have so far considered each MNIST data sample as a $(28\\times 28,)$-long 1d vector. This approach neglects any spatial structure in the image. On the other hand, we do know that in every one of the hand-written digits there are *local* spatial correlations between the pixels, which we would like to take advantage of to improve the accuracy of our classification model. To this end, we first need to reshape the training and test input data as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 28, 28, 1)\n",
      "Y_train shape: (60000, 10)\n",
      "\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# reshape data, depending on Keras backend\n",
    "if keras.backend.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    \n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print()\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can ask the question of whether a neural net can learn to recognize such local patterns. As we saw in Sec. X of the review, this can be achieved by using convolutional layers. Luckily, all we need to do is change the architecture of our DNN, i.e. introduce small changes to the function `create_model()`. We can also merge **Step 2** and **Step 3** for convenience: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_CNN():\n",
    "    # instantiate model\n",
    "    model = Sequential()\n",
    "    # add first convolutional layer with 10 filters (dimensionality of output space)\n",
    "    model.add(Conv2D(10, kernel_size=(5, 5),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    # add 2D pooling layer\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # add second convolutional layer with 20 filters\n",
    "    model.add(Conv2D(20, (5, 5), activation='relu'))\n",
    "    # apply dropout with rate 0.5\n",
    "    model.add(Dropout(0.5))\n",
    "    # add 2D pooling layer\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # flatten data\n",
    "    model.add(Flatten())\n",
    "    # add a dense all-to-all relu layer\n",
    "    model.add(Dense(20*4*4, activation='relu'))\n",
    "    # apply dropout with rate 0.5\n",
    "    model.add(Dropout(0.5))\n",
    "    # soft-max layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer='Adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the deep conv net (**Step 4**) and evaluating its performance (**Step 6**) proceeds exactly as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 39s 646us/step - loss: 0.2610 - acc: 0.9183 - val_loss: 0.0750 - val_acc: 0.9825\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 32s 533us/step - loss: 0.0932 - acc: 0.9709 - val_loss: 0.0551 - val_acc: 0.9876\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 32s 533us/step - loss: 0.0689 - acc: 0.9789 - val_loss: 0.0408 - val_acc: 0.9893\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 32s 533us/step - loss: 0.0622 - acc: 0.9808 - val_loss: 0.0378 - val_acc: 0.9907\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 32s 537us/step - loss: 0.0531 - acc: 0.9837 - val_loss: 0.0333 - val_acc: 0.9915\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 33s 554us/step - loss: 0.0495 - acc: 0.9854 - val_loss: 0.0339 - val_acc: 0.9909\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 39s 649us/step - loss: 0.0475 - acc: 0.9851 - val_loss: 0.0330 - val_acc: 0.9929\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 39s 648us/step - loss: 0.0423 - acc: 0.9870 - val_loss: 0.0289 - val_acc: 0.9931\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 38s 633us/step - loss: 0.0421 - acc: 0.9874 - val_loss: 0.0290 - val_acc: 0.9928\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 39s 646us/step - loss: 0.0374 - acc: 0.9884 - val_loss: 0.0253 - val_acc: 0.9922\n",
      "10000/10000 [==============================] - 3s 288us/step\n",
      "\n",
      "Test loss: 0.025273755778139458\n",
      "Test accuracy: 0.9922\n"
     ]
    }
   ],
   "source": [
    "# training parameters\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "# create the deep conv net\n",
    "model_CNN=create_CNN()\n",
    "\n",
    "# train CNN\n",
    "model_CNN.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "\n",
    "# evaliate model\n",
    "score = model_CNN.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
