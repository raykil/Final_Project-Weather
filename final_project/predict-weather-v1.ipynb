{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "520a53ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "from matplotlib_inline import backend_inline\n",
    "backend_inline.set_matplotlib_formats('retina')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45974c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import torch # pytorch package, allows using GPUs\n",
    "# fix seed\n",
    "seed=17\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d8a9771",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_usage = psutil.virtual_memory()\n",
    "mem_usage_start = mem_usage.used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b40747",
   "metadata": {},
   "source": [
    "# Step 1. Identify data and info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d3b4eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_file_info = pd.DataFrame({'filelist' : glob('data/tmax_train/*.h5')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6789c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_file_info['num'] = [int(file.split('/')[2].split('_')[-1].split('.')[0]) for file in X_train_file_info['filelist']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecd23896",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_file_info = X_train_file_info.sort_values('num')\n",
    "X_train_file_info.index = range(len(X_train_file_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "523e5c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_info = np.load('data/tmax_train/tmax_X_train_info.npz').get('arr_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64575373",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.load('data/tmax_train/tmax_y_train.npz', allow_pickle=True).get('arr_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "683d1c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free: 52.1%\n",
      "Total: 187.37G\n",
      "Used: 96.77G\n",
      "Used - Start: 0.27G\n"
     ]
    }
   ],
   "source": [
    "mem_usage = psutil.virtual_memory()\n",
    "\n",
    "print(f\"Free: {mem_usage.percent}%\")\n",
    "print(f\"Total: {mem_usage.total/(1024**3):.2f}G\")\n",
    "print(f\"Used: {mem_usage.used/(1024**3):.2f}G\")\n",
    "print(f\"Used - Start: {(mem_usage.used - mem_usage_start)/(1024**3):.2f}G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa51ca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_file_info = pd.DataFrame({'filelist' : glob('data/tmax_val/*.h5')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c661d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_file_info['num'] = [int(file.split('/')[2].split('_')[-1].split('.')[0]) for file in X_val_file_info['filelist']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b0c46f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_file_info = X_val_file_info.sort_values('num')\n",
    "X_val_file_info.index = range(len(X_val_file_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d6bfc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_info = np.load('data/tmax_val/tmax_X_val_info.npz').get('arr_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e550eb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = np.load('data/tmax_val/tmax_y_val.npz', allow_pickle=True).get('arr_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65455ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free: 52.1%\n",
      "Total: 187.37G\n",
      "Used: 96.80G\n",
      "Used - Start: 0.30G\n"
     ]
    }
   ],
   "source": [
    "mem_usage = psutil.virtual_memory()\n",
    "\n",
    "print(f\"Free: {mem_usage.percent}%\")\n",
    "print(f\"Total: {mem_usage.total/(1024**3):.2f}G\")\n",
    "print(f\"Used: {mem_usage.used/(1024**3):.2f}G\")\n",
    "print(f\"Used - Start: {(mem_usage.used - mem_usage_start)/(1024**3):.2f}G\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61de7839",
   "metadata": {},
   "source": [
    "# Step 2. Initialize the dataset with a data loader"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6883bb5a",
   "metadata": {},
   "source": [
    "from torchvision import datasets # load data\n",
    "\n",
    "class Weather_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Weather pytorch dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_type, transform=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_type (string): `train`, `validate` or `test`: creates data_loader\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        #from sklearn.model_selection import train_test_split\n",
    "        #import collections\n",
    "        #import pickle as pickle\n",
    "\n",
    "        # path to data directory\n",
    "        train_data_filelist = X_train_file_info['filelist'].tolist()\n",
    "\n",
    "    '''\n",
    "    # override __len__ and __getitem__ of the Dataset() class\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data[1])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        sample=(self.data[0][idx,...],self.data[1][idx])\n",
    "        if self.transform:\n",
    "            sample=self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "    '''\n",
    "    \n",
    "def load_data(kwargs):\n",
    "    # kwargs:  CUDA arguments, if enabled\n",
    "    # load and noralise train,test, and data\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        Ising_Dataset(data_type='train'),\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        Ising_Dataset(data_type='test'),\n",
    "        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    critical_loader = torch.utils.data.DataLoader(\n",
    "        Ising_Dataset(data_type='critical'),\n",
    "        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    return train_loader, test_loader, critical_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67b1c3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, data_path, y_data, batch_size=1000):\n",
    "        super().__init__()\n",
    "        with h5py.File(data_path, 'r') as f:\n",
    "            self.data_X_train = np.array(f['data'])\n",
    "        self.data_y_train = np.array(np.split(y_data, len(self.data_X_train)//batch_size))\n",
    "        self.data_X_train = np.array(np.split(self.data_X_train, len(self.data_X_train)//batch_size))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_X_train[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_X_train)\n",
    "\n",
    "def load_data(dataset_path, y_data, i):\n",
    "    # define dataset path\n",
    "    \n",
    "    # create dataset object\n",
    "    batch_size = 1000\n",
    "    dataset = WeatherDataset(dataset_path[i], y_data[i*1000:i*1000+1000], batch_size=batch_size)\n",
    "\n",
    "    # create data loader object\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67543026",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_chunk = load_data(X_train_file_info['filelist'].tolist(), y_train, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf9e9733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1000, 10, 11, 365)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_chunk.dataset.data_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bc6aa90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1000, 7)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_chunk.dataset.data_y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b499e359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free: 52.2%\n",
      "Total: 187.37G\n",
      "Used: 97.12G\n",
      "Used - Start: 0.61G\n"
     ]
    }
   ],
   "source": [
    "mem_usage = psutil.virtual_memory()\n",
    "\n",
    "print(f\"Free: {mem_usage.percent}%\")\n",
    "print(f\"Total: {mem_usage.total/(1024**3):.2f}G\")\n",
    "print(f\"Used: {mem_usage.used/(1024**3):.2f}G\")\n",
    "print(f\"Used - Start: {(mem_usage.used - mem_usage_start)/(1024**3):.2f}G\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39639ed2",
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        super().__init__()\n",
    "        with h5py.File(data_path[0], 'r') as f:\n",
    "            self.data_X_train = np.array(f['data'])\n",
    "        for file in data_path[1:]:\n",
    "            with h5py.File(file, 'r') as f:\n",
    "                self.data_X_train = np.append(self.data_X_train, np.array(f['data']), axis=0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_X_train[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_X_train)\n",
    "\n",
    "    \n",
    "    \n",
    "def load_data(dataset_path):\n",
    "    # define dataset path\n",
    "    \n",
    "\n",
    "    # create dataset object\n",
    "    dataset = WeatherDataset(dataset_path)\n",
    "\n",
    "    # create data loader object\n",
    "    batch_size = 1\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f6b7dc3",
   "metadata": {},
   "source": [
    "data_loader = load_data(X_train_file_info['filelist'].tolist())\n",
    "    \n",
    "    \n",
    "    # file_info is a pandas dataframe with filepaths for each data record and corresponding indices\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "0abecbdf",
   "metadata": {},
   "source": [
    "mem_usage = psutil.virtual_memory()\n",
    "\n",
    "print(f\"Free: {mem_usage.percent}%\")\n",
    "print(f\"Total: {mem_usage.total/(1024**3):.2f}G\")\n",
    "print(f\"Used: {mem_usage.used/(1024**3):.2f}G\")\n",
    "print(f\"Used - Start: {(mem_usage.used - mem_usage_start)/(1024**3):.2f}G\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "08a46fd7",
   "metadata": {},
   "source": [
    "data_loader.dataset.data_X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7a19a4",
   "metadata": {},
   "source": [
    "# Step 3. Build a simple architecture to explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9f468b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn # construct NN\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # 3D convolutional layers\n",
    "        self.conv1 = nn.Conv3d(in_channels=10, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv3d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Batch normalization layers\n",
    "        self.bn1 = nn.BatchNorm3d(num_features=16)\n",
    "        self.bn2 = nn.BatchNorm3d(num_features=32)\n",
    "        \n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool3d(kernel_size=(1,2,2), stride=(1,2,2))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(in_features=32*11*91*250, out_features=512) #32*5*182*128\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=7)\n",
    "        \n",
    "        # Dropout layer\n",
    "        #self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # ReLU activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size=400000, channels=10, depth=11, height=365, width=1)\n",
    "        \n",
    "        # First convolutional block\n",
    "        #print(x.shape)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        # Second convolutional block\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        # Flatten\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        # Fully connected layers with dropout\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        #x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Output shape: (batch_size=400000, num_classes=10)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b273798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train(model, train_loader, val_loader, num_epochs=10, learning_rate=0.001):\n",
    "    # Set device to GPU if available, else CPU\n",
    "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        \n",
    "        '''\n",
    "        # iterate over the batches\n",
    "        for batch_i in range(train_loader.dataset.data_X_train.shape[1]):\n",
    "            inputs = torch.Tensor(train_loader.dataset.data_X_train[:, batch_i, :, :, :].astype('int64')).reshape(10, 11, 365, 1)\n",
    "            labels = torch.Tensor(train_loader.dataset.data_y_train[:, batch_i, :].astype('int64'))\n",
    "\n",
    "            # Move data to device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track training loss and accuracy\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        '''\n",
    "        inputs = torch.Tensor(train_loader.dataset.data_X_train.astype('int64').reshape(1, 10, 11, 365, 1000))\n",
    "        labels = torch.Tensor(train_loader.dataset.data_y_train.astype('int64')[0, :, :].reshape(7, 1000))\n",
    "        # Move data to device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        print(outputs.shape)\n",
    "        print(labels.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track training loss and accuracy\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        \n",
    "        # Calculate average training loss and accuracy\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_accuracy = 100. * train_correct / len(train_loader.dataset)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        \n",
    "        # Disable gradient computation\n",
    "        with torch.no_grad():\n",
    "            '''\n",
    "            # Iterate over val loader\n",
    "            for inputs, labels in zip(val_loader.dataset.data_X_train, val_loader.dataset.data_y_train):\n",
    "                # Move data to device\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Track validation loss and accuracy\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "            '''\n",
    "            \n",
    "            inputs = torch.Tensor(val_loader.dataset.data_X_train.astype('int64').reshape(1, 10, 11, 365, 1000))\n",
    "            labels = torch.Tensor(val_loader.dataset.data_y_train.astype('int64')[0, :, :].reshape(7, 1000))\n",
    "            # Move data to device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Track validation loss and accuracy\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "        # Calculate average validation loss and accuracy\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_accuracy = 100. * val_correct / len(val_loader.dataset)\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(\"Epoch [{}/{}], Train Loss: {:.4f}, Train Acc: {:.2f}%, Val Loss: {:.4f}, Val Acc: {:.2f}%\"\n",
    "              .format(epoch+1, num_epochs, train_loss, train_accuracy, val_loss, val_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a1679f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loader(i):\n",
    "    return load_data(X_train_file_info['filelist'].tolist(), y_train, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7e1ba16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loader(i):\n",
    "    return load_data(X_val_file_info['filelist'].tolist(), y_val, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "73079ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "26259603",
   "metadata": {},
   "source": [
    "a = train_loader(0).dataset.data_X_train"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f9970e72",
   "metadata": {},
   "source": [
    "b = a.astype('int64')[:, 0, :, :, :]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb10ec95",
   "metadata": {},
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca84c3fd",
   "metadata": {},
   "source": [
    "c = a.astype('int64').reshape(1, 10, 11, 365, 1000)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f897ed0",
   "metadata": {},
   "source": [
    "model(torch.Tensor(c))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cfd450c3",
   "metadata": {},
   "source": [
    "d = train_loader(0).dataset.data_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a4e908e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free: 73.9%\n",
      "Total: 187.37G\n",
      "Used: 137.73G\n",
      "Used - Start: 41.22G\n"
     ]
    }
   ],
   "source": [
    "mem_usage = psutil.virtual_memory()\n",
    "\n",
    "print(f\"Free: {mem_usage.percent}%\")\n",
    "print(f\"Total: {mem_usage.total/(1024**3):.2f}G\")\n",
    "print(f\"Used: {mem_usage.used/(1024**3):.2f}G\")\n",
    "print(f\"Used - Start: {(mem_usage.used - mem_usage_start)/(1024**3):.2f}G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "9dd5c4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7])\n",
      "torch.Size([7, 1000])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1) to match target batch_size (7).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/scratch/7493164.1.biophys-gpu-pub/ipykernel_43973/3375603312.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch/7493164.1.biophys-gpu-pub/ipykernel_43973/769920739.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Backward pass and optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.9/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3025\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1) to match target batch_size (7)."
     ]
    }
   ],
   "source": [
    "train(model, train_loader(0), val_loader(0), num_epochs=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2d89ddc",
   "metadata": {},
   "source": [
    "Need to change some of the model structure so that each run of the model produces a (1, 7) output to match the labels. Then I can run it 1000 times. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "82914161",
   "metadata": {},
   "source": [
    "Got up to line 40something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ea87f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_usage = psutil.virtual_memory()\n",
    "\n",
    "print(f\"Free: {mem_usage.percent}%\")\n",
    "print(f\"Total: {mem_usage.total/(1024**3):.2f}G\")\n",
    "print(f\"Used: {mem_usage.used/(1024**3):.2f}G\")\n",
    "print(f\"Used - Start: {(mem_usage.used - mem_usage_start)/(1024**3):.2f}G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea618ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893c02b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b3766d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7458c603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn # construct NN\n",
    "\n",
    "class model(nn.Module):\n",
    "    # create convolutional net\n",
    "    def __init__(self, N=10, L=40):\n",
    "        # inherit attributes and methods of nn.Module\n",
    "        super(model, self).__init__()\t\n",
    "        # create convolutional layer with input depth 1 and output depth N\n",
    "        self.conv1 = nn.Conv3d(10, N, kernel_size=3, padding=1)\n",
    "        # batch norm layer takes Depth\n",
    "        self.bn1=nn.BatchNorm3d(N) \n",
    "        # create fully connected layer after maxpool operation reduced 40->18\n",
    "        self.fc1 = nn.Linear(1000, 7) \t\n",
    "        self.N=N\n",
    "        self.L=L\n",
    "        print(\"The number of neurons in CNN layer is %i\"%(N))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.Tensor(x)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #print(x.shape)  often useful to look at shapes for debugging\n",
    "        x = F.max_pool3d(x,3)\n",
    "        #print(x.shape)\n",
    "        x=self.bn1(x) # largely unnecessary and here just for pedagogical purposes\n",
    "        return F.log_softmax(self.fc1(x.view(-1,20*20*self.N)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3e4930e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, epoch):\n",
    "    CNN.train() # effects Dropout and BatchNorm layers\n",
    "    data = train_loader.dataset.data_X_train\n",
    "    target = train_loader.dataset.data_y_train\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = CNN(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % args.log_interval == 0:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, 0 * len(data), len(train_loader.dataset),\n",
    "            100. * 0 / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ae9301d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data_loader,verbose='Test'):\n",
    "    # these are very standard functions for evaluating data\n",
    "\n",
    "    CNN.eval() # effects Dropout and BatchNorm layers\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in data_loader:\n",
    "        output = CNN(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    print('\\n'+verbose+' set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(data_loader.dataset),\n",
    "        100. * correct / len(data_loader.dataset)))\n",
    "    accuracy=100. * correct / len(data_loader.dataset)\n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "05f6ea1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of neurons in CNN layer is 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [1, 10, 3, 3, 3], expected input[1, 1000, 10, 11, 365] to have 10 channels, but got 1000 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/scratch/7493164.1.biophys-gpu-pub/ipykernel_25423/2882857495.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#optimizer = optim.Adam(DNN.parameters(), lr=0.001, betas=(0.9, 0.999))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch/7493164.1.biophys-gpu-pub/ipykernel_25423/2780329421.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, epoch)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_y_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/7493164.1.biophys-gpu-pub/ipykernel_25423/1927643158.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m#print(x.shape)  often useful to look at shapes for debugging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             )\n\u001b[0;32m--> 608\u001b[0;31m         return F.conv3d(\n\u001b[0m\u001b[1;32m    609\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [1, 10, 3, 3, 3], expected input[1, 1000, 10, 11, 365] to have 10 channels, but got 1000 channels instead"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F # implements forward and backward definitions of an autograd operation\n",
    "import torch.optim as optim # different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc\n",
    "\n",
    "CNN = model(N=1)\n",
    "# negative log-likelihood (nll) loss for training: takes class labels NOT one-hot vectors!\n",
    "criterion = F.nll_loss\n",
    "# define optimizer\n",
    "optimizer = optim.SGD(CNN.parameters(), lr=0.001, momentum=0.001)\n",
    "#optimizer = optim.Adam(DNN.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "\n",
    "train(train_loader(0), 1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e94bc9d9",
   "metadata": {},
   "source": [
    "import argparse # handles arguments\n",
    "import sys; sys.argv=['']; del sys # required to use parser in jupyter notebooks\n",
    "\n",
    "# training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch Convmodel Ising Example')\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                    help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "args = parser.parse_args()\n",
    "args.epochs=5\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "cuda_kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5e7f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader, critical_loader=load_data(cuda_kwargs)\n",
    "\n",
    "next(iter(train_loader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dcf15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b43e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array=[]\n",
    "critical_array=[]\n",
    "\n",
    "# create array of depth of convolutional layer\n",
    "N_array=[1]\n",
    "\n",
    "# loop over depths\n",
    "for N in N_array:\n",
    "    CNN = model(N=N)\n",
    "\n",
    "    # negative log-likelihood (nll) loss for training: takes class labels NOT one-hot vectors!\n",
    "    criterion = F.nll_loss\n",
    "    # define optimizer\n",
    "    #optimizer = optim.SGD(CNN.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    optimizer = optim.Adam(DNN.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "\n",
    "    # train the CNN and test its performance at each epoch\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(epoch)\n",
    "        if epoch==args.epochs:\n",
    "            test_array.append(test(test_loader,verbose='Test'))\n",
    "        else:\n",
    "            test(test_loader,verbose='Test')\n",
    "    print(test_array)\n",
    "    print(critical_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
