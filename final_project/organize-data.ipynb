{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "520a53ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "from matplotlib_inline import backend_inline\n",
    "backend_inline.set_matplotlib_formats('retina')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45974c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import torch # pytorch package, allows using GPUs\n",
    "# fix seed\n",
    "seed=17\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b40747",
   "metadata": {},
   "source": [
    "# Step 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3bb4be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax_data = pd.read_csv('data/ghcnd_hcn_tmax.csv.gz', compression='gzip')\n",
    "tmax_data.replace(-9999, np.nan, inplace=True)\n",
    "#tmax_data.drop(columns=['latitude', 'longitude', 'elevation'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61de7839",
   "metadata": {},
   "source": [
    "# Step 2. Organize the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9568f94",
   "metadata": {},
   "source": [
    "After discussing with Pankaj, here is how I'm going to assemble the data.  \n",
    "\n",
    "- For each station, take a 3-D array (days x years x neighbor stations).  \n",
    "- I'm going to go with 10 years of data for each sample and the 10 nearest stations.\n",
    "    - Iterate through each week, choosing X to be the 10 previous years and y to be the subsequent week of temperatures for the station of interest.\n",
    "- Start with using just the 20th century data, leaving all 21st century data for validation and testing later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a29bebcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax_data_h20c = tmax_data[np.logical_and(tmax_data['year'] >= 1951, tmax_data['year'] <= 2001)]\n",
    "\n",
    "# remove any station that don't have continuous data from 1941 to 2001\n",
    "tmp = []\n",
    "counter = 0\n",
    "for station in tmax_data_h20c['station'].unique():\n",
    "    a = tmax_data_h20c[tmax_data_h20c['station'] == station]\n",
    "    if a['year'].nunique() < 51:\n",
    "        continue\n",
    "    elif a['year'].nunique() == 51:\n",
    "        tmp.append(a.copy().to_numpy())\n",
    "        counter += 1\n",
    "    elif a['year'].nunique() > 51:\n",
    "        continue\n",
    "\n",
    "tmax_data_h20c = pd.DataFrame(np.vstack(tmp), columns=tmax_data.columns)\n",
    "del(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40d12bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#station_info = pd.read_csv('data/ghcnd_hcn_tmax.csv.gz', compression='gzip')\n",
    "station_info = tmax_data_h20c.copy()\n",
    "#station_info = station_info[np.logical_and(station_info['year'] >= 1951, station_info['year'] <= 2001)]\n",
    "station_info = station_info[['station', 'latitude', 'longitude', 'elevation']].copy()\n",
    "station_info.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bcab61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax_data_h20c.drop(columns=['latitude', 'longitude', 'elevation'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ddbaa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_year_and_day(week):\n",
    "    start_year = 1961\n",
    "    ordered_day = week * 7\n",
    "    year = start_year + ordered_day // 365\n",
    "    day = ordered_day - (year - start_year)*365\n",
    "    return year, day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81ec16a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_stations(station, nearest=10):\n",
    "    global lat1, lon1, lat2, lon2, dist\n",
    "    nearest = 10\n",
    "    lat1, lon1 = station_info[station_info['station']==station].values[0][1:3]\n",
    "\n",
    "    lat2 = station_info['latitude'].astype(float)\n",
    "    lon2 = station_info['longitude'].astype(float)\n",
    "\n",
    "    dist = np.arccos((np.sin(lat1 * np.pi/180) * np.sin(lat2 * np.pi/180) + \\\n",
    "                     np.cos(lat1 * np.pi/180) * np.cos(lat2 * np.pi/180) * np.cos((lon2 - lon1) * np.pi/180)).round(10)) * 6371\n",
    "    close_stations = station_info.loc[dist.sort_values()[0:nearest].index, 'station']\n",
    "    return close_stations#, dist.sort_values()[0:nearest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fc24840",
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks = np.arange(0, 365*40//7, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "8194f17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 / 2085 \r"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i, station in enumerate(tmax_data_h20c['station'].unique()):\n",
    "    print(i+1, '/', len(tmax_data_h20c['station'].unique()), '\\r', end='')\n",
    "    for j, week in enumerate(weeks):\n",
    "        if (j+1) % 100 == 0:\n",
    "            print(j+1, '/', len(weeks), '\\r', end='')\n",
    "        nearest_stations = find_nearest_stations(station, nearest=10)\n",
    "        near_data = tmax_data_h20c[np.in1d(tmax_data_h20c['station'], nearest_stations.to_numpy())]\n",
    "        \n",
    "        end_year, end_day = find_year_and_day(week)\n",
    "        start_year = end_year - 10\n",
    "        start_day = end_day\n",
    "                \n",
    "        end_year_y, end_day_y = find_year_and_day(week+1)\n",
    "        \n",
    "        time_data_X = near_data[np.logical_and(near_data['year'] >= start_year, near_data['year'] <= end_year)].copy()\n",
    "        time_data_X.loc[time_data_X[time_data_X['year']==start_year].index, ['day' + str(day) for day in range(1, start_day+1)]] = np.nan\n",
    "        time_data_X.loc[time_data_X[time_data_X['year']==end_year].index, ['day' + str(day) for day in range(end_day+1, 366)]] = np.nan\n",
    "        \n",
    "        if end_day > 357:\n",
    "            time_data_y = near_data[np.logical_and(near_data['year'] >= end_year, near_data['year'] <= end_year_y)].copy() # 2 years for y that wraps\n",
    "            time_data_y = time_data_y[time_data_y['station']==station] # only select the target station for y\n",
    "            time_data_y1 = time_data_y.loc[:, ['day' + str(day) for day in range(end_day+1, 366)]] # first part of week\n",
    "            time_data_y2 = time_data_y.loc[:, ['day' + str(day) for day in range(1, end_day_y+1)]] # first part of week\n",
    "            time_data_y = np.hstack([time_data_y1.to_numpy(), time_data_y2.to_numpy()])\n",
    "        else:\n",
    "            time_data_y = near_data[near_data['year'] == end_year].copy() # 1 year for y that doesn't wrap\n",
    "            time_data_y = time_data_y[time_data_y['station']==station] # only select the target station for y\n",
    "            time_data_y = time_data_y.loc[:, ['day' + str(day) for day in range(end_day+1, end_day+8)]].to_numpy() # only select one week for y\n",
    "        if len(time_data_y[0]) != 7:\n",
    "            break\n",
    "        \n",
    "        stack_data = np.zeros((10, 11, 365))\n",
    "        for k, near_station in enumerate(nearest_stations.to_numpy()):\n",
    "            stack_data[k, :, :] = time_data_X[time_data_X['station']==near_station].loc[:, ['day' + str(day) for day in range(1, 366)]]\n",
    "        \n",
    "        X_train.append(stack_data)\n",
    "        y_train.append(time_data_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6d3f4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = []\n",
    "counter = 0\n",
    "for i, station in enumerate(tmax_data_h20c['station'].unique()):\n",
    "    for j, week in enumerate(weeks):\n",
    "        info.append([counter, station, find_year_and_day(week)[0], find_year_and_day(week)[1]])\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4a3724",
   "metadata": {},
   "source": [
    "# Step 3.  Save all training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "2f34e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('data/tmax_train/tmax_y_train.npz', np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cce04dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('data/tmax_train/tmax_X_train_info.npz', np.array(info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "e8935e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)//1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "26b85e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311 / 310 \r"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while i <= 310:\n",
    "    print(i+1, '/', 310, '\\r', end='')\n",
    "    with h5py.File('data/tmax_train/%X_train_i.h5'%(i*1000), 'w') as f:\n",
    "        dset = f.create_dataset('data', data=np.nan_to_num(np.array(X_train[i*1000:i*1000+1000]), nan=-8888).astype(int), compression='gzip')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360380fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9ff21d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
